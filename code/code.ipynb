{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine==2.8.0 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: pymongo==4.5.0 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.5.0)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/site-packages (from pymongo==4.5.0->-r requirements.txt (line 2)) (2.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from pprint import pprint \n",
    "from pymongo import MongoClient, version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DbConnector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbConnector:\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB server on the Ubuntu virtual machine.\n",
    "    Connector needs HOST, USER and PASSWORD to connect.\n",
    "\n",
    "    Example:\n",
    "    HOST = \"tdt4225-00.idi.ntnu.no\" // Your server IP address/domain name\n",
    "    USER = \"testuser\" // This is the user you created and added privileges for\n",
    "    PASSWORD = \"test123\" // The password you set for said user\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 DATABASE='my_db',\n",
    "                 HOST=\"tdt4225-19.idi.ntnu.no\",\n",
    "                 USER=\"team19\",\n",
    "                 PASSWORD=\"team19*\"):\n",
    "        uri = \"mongodb://%s:%s@%s/%s\" % (USER, PASSWORD, HOST, DATABASE)\n",
    "        # Connect to the databases\n",
    "        try:\n",
    "            self.client = MongoClient('mongodb://team19:team19*@tdt4225-19.idi.ntnu.no:27017/my_db')\n",
    "            self.db = self.client[DATABASE]\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: Failed to connect to db:\", e)\n",
    "\n",
    "        # get database information\n",
    "        print(\"You are connected to the database:\", self.db.name)\n",
    "        print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "    def close_connection(self):\n",
    "        # close the cursor\n",
    "        # close the DB connection\n",
    "        self.client.close()\n",
    "        print(\"\\n-----------------------------------------------\")\n",
    "        print(\"Connection to %s-db is closed\" % self.db.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to the database: my_db\n",
      "-----------------------------------------------\n",
      "\n",
      "Created collection:  Collection(Database(MongoClient(host=['tdt4225-19.idi.ntnu.no:27017'], document_class=dict, tz_aware=False, connect=True), 'my_db'), 'Person')\n",
      "[]\n",
      "{'_id': 1,\n",
      " 'courses': [{'code': 'TDT4225',\n",
      "              'name': ' Very Large, Distributed Data Volumes'},\n",
      "             {'code': 'BOI1001', 'name': ' How to become a boi or boierinnaa'}],\n",
      " 'name': 'Bobby'}\n",
      "{'_id': 2,\n",
      " 'courses': [{'code': 'TDT02', 'name': ' Advanced, Distributed Systems'}],\n",
      " 'name': 'Bobby'}\n",
      "{'_id': 3, 'name': 'Bobby'}\n",
      "[]\n",
      "\n",
      "-----------------------------------------------\n",
      "Connection to my_db-db is closed\n"
     ]
    }
   ],
   "source": [
    "class ExampleProgram:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.connection = DbConnector()\n",
    "        self.client = self.connection.client\n",
    "        self.db = self.connection.db\n",
    "\n",
    "    def create_coll(self, collection_name):\n",
    "        collection = self.db.create_collection(collection_name)    \n",
    "        print('Created collection: ', collection)\n",
    "\n",
    "    def insert_documents(self, collection_name):\n",
    "        docs = [\n",
    "            {\n",
    "                \"_id\": 1,\n",
    "                \"name\": \"Bobby\",\n",
    "                \"courses\": \n",
    "                    [\n",
    "                    {'code':'TDT4225', 'name': ' Very Large, Distributed Data Volumes'},\n",
    "                    {'code':'BOI1001', 'name': ' How to become a boi or boierinnaa'}\n",
    "                    ] \n",
    "            },\n",
    "            {\n",
    "                \"_id\": 2,\n",
    "                \"name\": \"Bobby\",\n",
    "                \"courses\": \n",
    "                    [\n",
    "                    {'code':'TDT02', 'name': ' Advanced, Distributed Systems'},\n",
    "                    ] \n",
    "            },\n",
    "            {\n",
    "                \"_id\": 3,\n",
    "                \"name\": \"Bobby\",\n",
    "            }\n",
    "        ]  \n",
    "        collection = self.db[collection_name]\n",
    "        collection.insert_many(docs)\n",
    "        \n",
    "    def fetch_documents(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        documents = collection.find({})\n",
    "        for doc in documents: \n",
    "            pprint(doc)\n",
    "        \n",
    "\n",
    "    def drop_coll(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        collection.drop()\n",
    "\n",
    "        \n",
    "    def show_coll(self):\n",
    "        collections = self.client['test'].list_collection_names()\n",
    "        print(collections)\n",
    "         \n",
    "\n",
    "\n",
    "def main():\n",
    "    program = None\n",
    "    try:\n",
    "        program = ExampleProgram()\n",
    "        program.create_coll(collection_name=\"Person\")\n",
    "        program.show_coll()\n",
    "        program.insert_documents(collection_name=\"Person\")\n",
    "        program.fetch_documents(collection_name=\"Person\")\n",
    "        program.drop_coll(collection_name=\"Person\")\n",
    "        # program.drop_coll(collection_name='person')\n",
    "        # program.drop_coll(collection_name='users')\n",
    "        # Check that the table is dropped\n",
    "        program.show_coll()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to use database:\", e)\n",
    "    finally:\n",
    "        if program:\n",
    "            program.connection.close_connection()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/dataset/dataset/Data\n"
     ]
    }
   ],
   "source": [
    "# we go to the Data folder\n",
    "os.chdir(\"..\")\n",
    "path = os.getcwd()\n",
    "os.chdir(\"/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/dataset/dataset/Data\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_user_dataframe():\n",
    "    # we get the list of all the diferent directory names (users ids) and we sort the list\n",
    "    usersID =  os.listdir()\n",
    "    usersID.sort()\n",
    "\n",
    "    # we go back to the dataset directory and we read the labeled_ids.txt file\n",
    "    os.chdir(\"..\")\n",
    "    with open('labeled_ids.txt') as f:\n",
    "        labeled_ids = f.readlines()\n",
    "    f.close()\n",
    "    os.chdir(\"/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/dataset/dataset/Data\")\n",
    "        \n",
    "    # we delete the \\n in each string\n",
    "    for i in range (0, len(labeled_ids)):\n",
    "        labeled_ids[i] = labeled_ids[i].strip()\n",
    "\n",
    "    # we check if each user has a label or not and we save the info in a list\n",
    "    # the indexes of has_labels and id lists are correponding\n",
    "    has_labels = []\n",
    "    for i in usersID : \n",
    "        if i in labeled_ids : \n",
    "            has_labels.append(True)\n",
    "        else:\n",
    "            has_labels.append(False)\n",
    "\n",
    "    user_table = {'user_id': usersID, 'has_labels': has_labels}\n",
    "    user_dataframe = pd.DataFrame(user_table)\n",
    "    return user_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_labels_txt_file() :\n",
    "    user_id_list = []\n",
    "    transportation_mode_list = []\n",
    "    start_date_time_list = []\n",
    "    end_date_time_list = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(os.getcwd()):\n",
    "        for filename in filenames:\n",
    "            # if there is a labels.txt file, we save the info\n",
    "            if filename.endswith('.txt'):\n",
    "                try :\n",
    "                    with open(os.path.join(dirpath, filename)) as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                        # skip the header\n",
    "                        lines = lines[1:]\n",
    "\n",
    "                        for line in lines:\n",
    "                            \n",
    "                            # we get the info of one line\n",
    "                            data = line.split()\n",
    "\n",
    "                            # we save each information into the correct list\n",
    "                            last_directory_name = os.path.basename(dirpath)\n",
    "                            user_id_list.append(last_directory_name)\n",
    "\n",
    "                            transportation_mode_list.append(data[4])\n",
    "\n",
    "                            start_date = data[0]\n",
    "                            start_time = data[1]\n",
    "                            end_date = data[2]\n",
    "                            end_time = data[3]\n",
    "                            start_datetime_str = start_date + \" \" + start_time\n",
    "                            end_datetime_str = end_date + \" \" + end_time\n",
    "                            start_datetime_str = start_datetime_str.replace('/', '-')\n",
    "                            end_datetime_str = end_datetime_str.replace('/', '-')\n",
    "                            combined_start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                            combined_end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                            start_date_time_list.append(combined_start_datetime)\n",
    "                            end_date_time_list.append(combined_end_datetime)\n",
    "                    f.close()\n",
    "\n",
    "                # error handling\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"the file {filename} doesn't exist.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "    return user_id_list, transportation_mode_list, start_date_time_list, end_date_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_plt_file() :\n",
    "    activity = 0\n",
    "    activity_id_list = []\n",
    "    user_id_list = []\n",
    "    lat_list = []\n",
    "    long_list = []\n",
    "    altitude_list = []\n",
    "    date_days_list = []\n",
    "    current_date_time_list = []\n",
    "    start_date_time_list = []\n",
    "    end_date_time_list = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(os.getcwd()):\n",
    "        for filename in filenames:\n",
    "\n",
    "            # we get the information of each plt file\n",
    "            if filename.endswith('.plt'):\n",
    "                try :\n",
    "                    with open(os.path.join(dirpath, filename)) as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                        # skip the first 6 lines\n",
    "                        lines = lines[6:]\n",
    "\n",
    "                        # check the length of the plt file\n",
    "                        if len(lines) <= 2500:\n",
    "\n",
    "                            # we get the start and end date of each plt file\n",
    "                            start_line= lines[0].split(',')\n",
    "                            start_date = start_line[5]\n",
    "                            start_time = start_line[6]\n",
    "                            start_datetime = start_date + ' ' + start_time\n",
    "                            start_datetime = start_datetime.rstrip('\\n')\n",
    "                            start_datetime = datetime.strptime(start_datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                            end_line = lines[len(lines)-1].split(',')\n",
    "                            end_date = end_line[5]\n",
    "                            end_time = end_line[6]\n",
    "                            end_datetime = end_date + ' ' + end_time\n",
    "                            end_datetime = end_datetime.rstrip('\\n')\n",
    "                            end_datetime = datetime.strptime(end_datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "                            # we save the information of each line of the plt file\n",
    "                            for line in lines:\n",
    "\n",
    "                                data = line.split(',')\n",
    "\n",
    "                                activity_id_list.append(activity)\n",
    "\n",
    "                                parent_directory = os.path.dirname(dirpath)\n",
    "                                directory_name = os.path.basename(parent_directory) \n",
    "                                user_id_list.append(directory_name)\n",
    "\n",
    "                                lat_list.append(float(data[0]))\n",
    "                                long_list.append(float(data[1]))\n",
    "                                altitude_list.append(int(float(data[3])))\n",
    "                                date_days_list.append(float(data[4]))\n",
    "\n",
    "                                date = data[5]\n",
    "                                time = data[6]\n",
    "                                datetime_draft = date + ' ' + time\n",
    "                                datetime_draft = datetime_draft.rstrip('\\n')\n",
    "                                combined_datetime = datetime.strptime(datetime_draft, \"%Y-%m-%d %H:%M:%S\")\n",
    "                                current_date_time_list.append(combined_datetime)\n",
    "\n",
    "                                start_date_time_list.append(start_datetime)\n",
    "                                end_date_time_list.append(end_datetime)\n",
    "\n",
    "                    f.close()\n",
    "                    activity +=1\n",
    "                  \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"the file {filename} doesn't exist.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    \n",
    "    return activity_id_list, user_id_list, lat_list, long_list, altitude_list, date_days_list, current_date_time_list, start_date_time_list, end_date_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_activity_and_trackpoint_dataframe():    \n",
    "    # getting info from txt files\n",
    "    user_id_list, transportation_mode_list, start_date_time_list, end_date_time_list = get_info_from_labels_txt_file()\n",
    "    labels_txt = {'user_id': user_id_list, 'transportation_mode': transportation_mode_list, 'start_datetime': start_date_time_list, 'end_datetime': end_date_time_list}\n",
    "    labels_txt_df = pd.DataFrame(labels_txt)\n",
    "\n",
    "    # getting info from plt files\n",
    "    activity_id_list, user_id_list_2, lat_list, long_list, altitude_list, date_days_list, current_date_time_list, start_date_time_list2, end_date_time_list2 = get_info_from_plt_file()\n",
    "    plt = {'activity_id': activity_id_list, 'user_id': user_id_list_2, 'lat': lat_list, 'long': long_list, 'altitude': altitude_list, 'date_days': date_days_list, \n",
    "           'current_date_time': current_date_time_list, 'start_datetime': start_date_time_list2, 'end_datetime': end_date_time_list2}\n",
    "    plt_df = pd.DataFrame(plt)\n",
    "\n",
    "    # merging both dataframes\n",
    "    merged_df = pd.merge(plt_df, labels_txt_df, on=['user_id', 'start_datetime', 'end_datetime'], how='left')\n",
    "\n",
    "    # creating activity table\n",
    "    activity_table = merged_df[['activity_id','user_id', 'transportation_mode', 'start_datetime', 'end_datetime']]\n",
    "    activity_table = activity_table.fillna(\"missing\")\n",
    "    activity_table['start_datetime'] = activity_table['start_datetime'].astype(str) # converting into string to be able to insert into the sql table\n",
    "    activity_table['end_datetime'] = activity_table['end_datetime'].astype(str)\n",
    "    activity_table = activity_table.drop_duplicates()\n",
    "\n",
    "    # creating trackpoint table\n",
    "    trackpoint_table = merged_df[['activity_id','lat', 'long', 'altitude', 'date_days', 'current_date_time']]\n",
    "    trackpoint_table.rename(columns={'current_date_time': 'date_time'}, inplace=True)\n",
    "    trackpoint_table['id'] = range(1, len(trackpoint_table) + 1)\n",
    "    trackpoint_table = trackpoint_table[['id'] + [col for col in trackpoint_table.columns if col != 'id']]\n",
    "    trackpoint_table['date_time'] = trackpoint_table['date_time'].astype(str)\n",
    "\n",
    "    return activity_table, trackpoint_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task1:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.connection = DbConnector()\n",
    "        self.client = self.connection.client\n",
    "        self.db = self.connection.db\n",
    "\n",
    "    def create_coll(self, collection_name):\n",
    "        collection = self.db.create_collection(collection_name)    \n",
    "        print('Created collection: ', collection)\n",
    "\n",
    "    def insert_activity_data(self, user_table, activity_table):\n",
    "        merged_df = pd.merge(user_table, activity_table, on=['user_id'])\n",
    "\n",
    "        result_list = []\n",
    "        for index, row in merged_df.iterrows():\n",
    "            user_info = {'user_id': row['user_id'], 'has_labels': row['has_labels']}\n",
    "            entry = {\n",
    "                '_id': index,\n",
    "                'activity_id': row['activity_id'],\n",
    "                'user_info': user_info,\n",
    "                'transportation_mode': row['transportation_mode'],\n",
    "                'start_datetime': row['start_datetime'],\n",
    "                'end_datetime': row['end_datetime']\n",
    "            }\n",
    "            result_list.append(entry)\n",
    "        \n",
    "        collection = self.db[\"Activity\"]\n",
    "        collection.insert_many(result_list)\n",
    "\n",
    "    def insert_trackpoint_data(self, activity_table, trackpoint_table):\n",
    "        \n",
    "        result_list = []\n",
    "        for index, row in trackpoint_table.iterrows():\n",
    "            entry = {\n",
    "                '_id': row['id'],\n",
    "                'activity_id': row['activity_id'],\n",
    "                'lat': row['lat'],\n",
    "                'lon': row['long'],\n",
    "                'altitude': row['altitude'],\n",
    "                'date_days': row['date_days'],\n",
    "                'date_time': row['date_time']\n",
    "            }\n",
    "            result_list.append(entry)\n",
    "        \n",
    "        collection = self.db[\"TrackPoint\"]\n",
    "        collection.insert_many(result_list)\n",
    "\n",
    "    def compute_aggregated_query(self, collection_name, pipeline):\n",
    "        collection = self.db[collection_name]\n",
    "        result = collection.aggregate(pipeline)\n",
    "        for i in result: \n",
    "            pprint(i)\n",
    "        \n",
    "    def fetch_documents(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        documents = collection.find({})\n",
    "        for doc in documents: \n",
    "            pprint(doc)\n",
    "        \n",
    "    def drop_coll(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        collection.drop()\n",
    "\n",
    "        \n",
    "    def show_coll(self):\n",
    "        collections = self.client['my_db'].list_collection_names()\n",
    "        print(collections)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to the database: my_db\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "Connection to my_db-db is closed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    program = None\n",
    "    try:\n",
    "        activity_table, trackpoint_table = creating_activity_and_trackpoint_dataframe()\n",
    "        user_table = creating_user_dataframe()\n",
    "        \n",
    "        program = Task1()\n",
    "\n",
    "        program.drop_coll(collection_name=\"Activity\")\n",
    "        program.drop_coll(collection_name=\"TrackPoint\")\n",
    "\n",
    "        program.create_coll(collection_name=\"Activity\")\n",
    "        program.create_coll(collection_name=\"TrackPoint\")\n",
    "        program.show_coll()\n",
    "        \n",
    "        program.insert_activity_data(user_table, activity_table)\n",
    "        program.insert_trackpoint_data(activity_table, trackpoint_table)\n",
    "\n",
    "        program.fetch_documents(collection_name=\"Activity\")\n",
    "        program.fetch_documents(collection_name=\"TrackPoint\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to use database:\", e)\n",
    "    finally:\n",
    "        if program:\n",
    "            program.connection.close_connection()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to the database: my_db\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "connection = DbConnector()\n",
    "client = connection.client\n",
    "db = connection.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '128', 'count': 2102}\n",
      "{'_id': '153', 'count': 1793}\n",
      "{'_id': '025', 'count': 715}\n",
      "{'_id': '163', 'count': 704}\n",
      "{'_id': '062', 'count': 691}\n",
      "{'_id': '144', 'count': 563}\n",
      "{'_id': '041', 'count': 399}\n",
      "{'_id': '085', 'count': 364}\n",
      "{'_id': '004', 'count': 346}\n",
      "{'_id': '140', 'count': 345}\n",
      "{'_id': '167', 'count': 320}\n",
      "{'_id': '068', 'count': 280}\n",
      "{'_id': '017', 'count': 265}\n",
      "{'_id': '003', 'count': 261}\n",
      "{'_id': '014', 'count': 236}\n",
      "{'_id': '126', 'count': 215}\n",
      "{'_id': '030', 'count': 210}\n",
      "{'_id': '112', 'count': 208}\n",
      "{'_id': '011', 'count': 201}\n",
      "{'_id': '039', 'count': 198}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$user_info.user_id\",\n",
    "            \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 20\n",
    "    }\n",
    "]\n",
    "result = list(db.Activity.aggregate(pipeline))\n",
    "\n",
    "for doc in result:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 2008, 'count': 5895}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"year\": { \"$year\": { \"$dateFromString\": { \"dateString\": \"$start_datetime\" } }}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$year\",\n",
    "            \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": { \"count\": -1 }\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "result = list(db.Activity.aggregate(pipeline))\n",
    "\n",
    "for doc in result:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 2009, 'total_hours': 11612.423888888889}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"start_datetime\": {\n",
    "                \"$dateFromString\": {\n",
    "                    \"dateString\": \"$start_datetime\",\n",
    "                    \"format\": \"%Y-%m-%d %H:%M:%S\"\n",
    "                }\n",
    "            },\n",
    "            \"end_datetime\": {\n",
    "                \"$dateFromString\": {\n",
    "                    \"dateString\": \"$end_datetime\",\n",
    "                    \"format\": \"%Y-%m-%d %H:%M:%S\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"year\": { \"$year\": \"$start_datetime\" },\n",
    "            \"duration\": {\n",
    "                \"$divide\": [\n",
    "                    { \"$subtract\": [\"$end_datetime\", \"$start_datetime\"] },\n",
    "                    1000 * 60 * 60\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$year\",\n",
    "            \"total_hours\": { \"$sum\": \"$duration\" }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": { \"total_hours\": -1 }\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "result = list(db.Activity.aggregate(pipeline))\n",
    "\n",
    "for doc in result:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline8 = [\n",
    "               {\n",
    "            \"$sort\": { \"trackpoint_id\": 1 }\n",
    "        },\n",
    "        {\n",
    "            \"$group\": {\n",
    "            \"_id\": \"$activity_id\",\n",
    "            \"list_dates\": {\"$push\": \"$date_time\"},\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$lookup\": {\n",
    "            \"from\": \"Activity\",\n",
    "            \"localField\": \"_id\",\n",
    "            \"foreignField\": \"activity_id\",\n",
    "            \"as\": \"activity\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"user_id\" : \"$activity.user_info.user_id\",\n",
    "                \"activity_id\": \"$_id\",\n",
    "                \"list_dates\": \"$list_dates\"\n",
    "        }\n",
    "        }\n",
    "        ]\n",
    "result8 = list(db.TrackPoint.aggregate(pipeline8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 135 has 21 invalid activities.\n",
      "User 132 has 3 invalid activities.\n",
      "User 104 has 351 invalid activities.\n",
      "User 103 has 44 invalid activities.\n",
      "User 168 has 36 invalid activities.\n",
      "User 157 has 65 invalid activities.\n",
      "User 150 has 60 invalid activities.\n",
      "User 159 has 7 invalid activities.\n",
      "User 166 has 4 invalid activities.\n",
      "User 161 has 7 invalid activities.\n",
      "User 102 has 16 invalid activities.\n",
      "User 105 has 35 invalid activities.\n",
      "User 133 has 18 invalid activities.\n",
      "User 134 has 83 invalid activities.\n",
      "User 158 has 28 invalid activities.\n",
      "User 167 has 334 invalid activities.\n",
      "User 151 has 4 invalid activities.\n",
      "User 169 has 12 invalid activities.\n",
      "User 024 has 75 invalid activities.\n",
      "User 023 has 30 invalid activities.\n",
      "User 015 has 138 invalid activities.\n",
      "User 012 has 163 invalid activities.\n",
      "User 079 has 3 invalid activities.\n",
      "User 046 has 26 invalid activities.\n",
      "User 041 has 389 invalid activities.\n",
      "User 048 has 5 invalid activities.\n",
      "User 077 has 11 invalid activities.\n",
      "User 083 has 56 invalid activities.\n",
      "User 084 has 230 invalid activities.\n",
      "User 070 has 16 invalid activities.\n",
      "User 013 has 69 invalid activities.\n",
      "User 014 has 328 invalid activities.\n",
      "User 022 has 191 invalid activities.\n",
      "User 025 has 426 invalid activities.\n",
      "User 071 has 64 invalid activities.\n",
      "User 085 has 384 invalid activities.\n",
      "User 082 has 65 invalid activities.\n",
      "User 076 has 33 invalid activities.\n",
      "User 040 has 52 invalid activities.\n",
      "User 078 has 26 invalid activities.\n",
      "User 047 has 19 invalid activities.\n",
      "User 065 has 46 invalid activities.\n",
      "User 091 has 110 invalid activities.\n",
      "User 096 has 67 invalid activities.\n",
      "User 062 has 397 invalid activities.\n",
      "User 054 has 8 invalid activities.\n",
      "User 053 has 39 invalid activities.\n",
      "User 098 has 25 invalid activities.\n",
      "User 038 has 266 invalid activities.\n",
      "User 007 has 136 invalid activities.\n",
      "User 000 has 445 invalid activities.\n",
      "User 009 has 114 invalid activities.\n",
      "User 036 has 126 invalid activities.\n",
      "User 031 has 7 invalid activities.\n",
      "User 052 has 138 invalid activities.\n",
      "User 099 has 113 invalid activities.\n",
      "User 055 has 28 invalid activities.\n",
      "User 063 has 27 invalid activities.\n",
      "User 097 has 27 invalid activities.\n",
      "User 090 has 15 invalid activities.\n",
      "User 064 has 10 invalid activities.\n",
      "User 030 has 511 invalid activities.\n",
      "User 008 has 33 invalid activities.\n",
      "User 037 has 279 invalid activities.\n",
      "User 001 has 116 invalid activities.\n",
      "User 039 has 427 invalid activities.\n",
      "User 006 has 49 invalid activities.\n",
      "User 174 has 188 invalid activities.\n",
      "User 180 has 2 invalid activities.\n",
      "User 173 has 11 invalid activities.\n",
      "User 145 has 17 invalid activities.\n",
      "User 142 has 200 invalid activities.\n",
      "User 129 has 41 invalid activities.\n",
      "User 111 has 251 invalid activities.\n",
      "User 118 has 31 invalid activities.\n",
      "User 127 has 8 invalid activities.\n",
      "User 144 has 551 invalid activities.\n",
      "User 172 has 21 invalid activities.\n",
      "User 181 has 49 invalid activities.\n",
      "User 175 has 20 invalid activities.\n",
      "User 121 has 15 invalid activities.\n",
      "User 119 has 79 invalid activities.\n",
      "User 126 has 242 invalid activities.\n",
      "User 110 has 48 invalid activities.\n",
      "User 128 has 1288 invalid activities.\n",
      "User 117 has 3 invalid activities.\n",
      "User 153 has 1154 invalid activities.\n",
      "User 154 has 28 invalid activities.\n",
      "User 162 has 27 invalid activities.\n",
      "User 165 has 4 invalid activities.\n",
      "User 131 has 59 invalid activities.\n",
      "User 136 has 34 invalid activities.\n",
      "User 109 has 9 invalid activities.\n",
      "User 100 has 5 invalid activities.\n",
      "User 107 has 2 invalid activities.\n",
      "User 138 has 27 invalid activities.\n",
      "User 164 has 20 invalid activities.\n",
      "User 163 has 559 invalid activities.\n",
      "User 155 has 70 invalid activities.\n",
      "User 152 has 4 invalid activities.\n",
      "User 106 has 66 invalid activities.\n",
      "User 139 has 50 invalid activities.\n",
      "User 101 has 166 invalid activities.\n",
      "User 108 has 28 invalid activities.\n",
      "User 130 has 18 invalid activities.\n",
      "User 089 has 116 invalid activities.\n",
      "User 042 has 118 invalid activities.\n",
      "User 045 has 19 invalid activities.\n",
      "User 087 has 3 invalid activities.\n",
      "User 073 has 30 invalid activities.\n",
      "User 074 has 19 invalid activities.\n",
      "User 080 has 8 invalid activities.\n",
      "User 020 has 27 invalid activities.\n",
      "User 027 has 14 invalid activities.\n",
      "User 018 has 97 invalid activities.\n",
      "User 011 has 38 invalid activities.\n",
      "User 016 has 61 invalid activities.\n",
      "User 029 has 37 invalid activities.\n",
      "User 081 has 42 invalid activities.\n",
      "User 075 has 8 invalid activities.\n",
      "User 072 has 6 invalid activities.\n",
      "User 086 has 19 invalid activities.\n",
      "User 044 has 83 invalid activities.\n",
      "User 088 has 13 invalid activities.\n",
      "User 043 has 73 invalid activities.\n",
      "User 017 has 493 invalid activities.\n",
      "User 028 has 91 invalid activities.\n",
      "User 010 has 154 invalid activities.\n",
      "User 026 has 51 invalid activities.\n",
      "User 019 has 65 invalid activities.\n",
      "User 021 has 72 invalid activities.\n",
      "User 003 has 849 invalid activities.\n",
      "User 004 has 1173 invalid activities.\n",
      "User 032 has 37 invalid activities.\n",
      "User 035 has 124 invalid activities.\n",
      "User 095 has 20 invalid activities.\n",
      "User 061 has 41 invalid activities.\n",
      "User 066 has 40 invalid activities.\n",
      "User 092 has 315 invalid activities.\n",
      "User 059 has 8 invalid activities.\n",
      "User 050 has 17 invalid activities.\n",
      "User 057 has 60 invalid activities.\n",
      "User 068 has 327 invalid activities.\n",
      "User 034 has 241 invalid activities.\n",
      "User 033 has 3 invalid activities.\n",
      "User 005 has 127 invalid activities.\n",
      "User 002 has 296 invalid activities.\n",
      "User 056 has 11 invalid activities.\n",
      "User 069 has 65 invalid activities.\n",
      "User 051 has 146 invalid activities.\n",
      "User 093 has 4 invalid activities.\n",
      "User 067 has 83 invalid activities.\n",
      "User 058 has 25 invalid activities.\n",
      "User 060 has 1 invalid activities.\n",
      "User 094 has 60 invalid activities.\n",
      "User 112 has 140 invalid activities.\n",
      "User 115 has 183 invalid activities.\n",
      "User 123 has 21 invalid activities.\n",
      "User 124 has 12 invalid activities.\n",
      "User 170 has 2 invalid activities.\n",
      "User 141 has 1 invalid activities.\n",
      "User 146 has 28 invalid activities.\n",
      "User 179 has 42 invalid activities.\n",
      "User 125 has 69 invalid activities.\n",
      "User 122 has 21 invalid activities.\n",
      "User 114 has 4 invalid activities.\n",
      "User 113 has 1 invalid activities.\n",
      "User 147 has 64 invalid activities.\n",
      "User 140 has 195 invalid activities.\n",
      "User 176 has 24 invalid activities.\n",
      "User 171 has 14 invalid activities.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "# Function to check if two timestamps have a deviation of at least 5 minutes\n",
    "def is_invalid_timestamp(timestamp1, timestamp2):\n",
    "    time_format = '%Y-%m-%d %H:%M:%S'\n",
    "    dt1 = datetime.strptime(timestamp1, time_format)\n",
    "    dt2 = datetime.strptime(timestamp2, time_format)\n",
    "    return abs((dt2 - dt1).total_seconds()) >= 300  # 5 minutes in seconds\n",
    "\n",
    "# Create a dictionary to store user IDs and the count of invalid activities\n",
    "invalid_activities = {}\n",
    "\n",
    "for item in result8:\n",
    "    user_id = item['user_id'][0]\n",
    "    trackpoints = item['list_dates']\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for i in range(1, len(trackpoints)):\n",
    "        if is_invalid_timestamp(trackpoints[i - 1], trackpoints[i]):\n",
    "            invalid_count += 1\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        if user_id in invalid_activities:\n",
    "            invalid_activities[user_id] += invalid_count\n",
    "        else:\n",
    "            invalid_activities[user_id] = invalid_count\n",
    "\n",
    "# Print users with invalid activities and the number of invalid activities\n",
    "for user_id, count in invalid_activities.items():\n",
    "    print(f\"User {user_id} has {count} invalid activities.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------\n",
      "Connection to my_db-db is closed\n"
     ]
    }
   ],
   "source": [
    "connection.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
