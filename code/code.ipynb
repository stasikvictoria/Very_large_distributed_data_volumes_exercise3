{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: haversine==2.8.0 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: pymongo==4.5.0 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.5.0)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /usr/local/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (0.9.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.11/site-packages (from pymongo==4.5.0->-r requirements.txt (line 2)) (2.4.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.11 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "from pprint import pprint \n",
    "from pymongo import MongoClient, version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DbConnector.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbConnector:\n",
    "    \"\"\"\n",
    "    Connects to the MongoDB server on the Ubuntu virtual machine.\n",
    "    Connector needs HOST, USER and PASSWORD to connect.\n",
    "\n",
    "    Example:\n",
    "    HOST = \"tdt4225-00.idi.ntnu.no\" // Your server IP address/domain name\n",
    "    USER = \"testuser\" // This is the user you created and added privileges for\n",
    "    PASSWORD = \"test123\" // The password you set for said user\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 DATABASE='my_db',\n",
    "                 HOST=\"tdt4225-19.idi.ntnu.no\",\n",
    "                 USER=\"team19\",\n",
    "                 PASSWORD=\"team19*\"):\n",
    "        uri = \"mongodb://%s:%s@%s/%s\" % (USER, PASSWORD, HOST, DATABASE)\n",
    "        # Connect to the databases\n",
    "        try:\n",
    "            self.client = MongoClient('mongodb://team19:team19*@tdt4225-19.idi.ntnu.no:27017/my_db')\n",
    "            self.db = self.client[DATABASE]\n",
    "        except Exception as e:\n",
    "            print(\"ERROR: Failed to connect to db:\", e)\n",
    "\n",
    "        # get database information\n",
    "        print(\"You are connected to the database:\", self.db.name)\n",
    "        print(\"-----------------------------------------------\\n\")\n",
    "\n",
    "    def close_connection(self):\n",
    "        # close the cursor\n",
    "        # close the DB connection\n",
    "        self.client.close()\n",
    "        print(\"\\n-----------------------------------------------\")\n",
    "        print(\"Connection to %s-db is closed\" % self.db.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to the database: my_db\n",
      "-----------------------------------------------\n",
      "\n",
      "Created collection:  Collection(Database(MongoClient(host=['tdt4225-19.idi.ntnu.no:27017'], document_class=dict, tz_aware=False, connect=True), 'my_db'), 'Person')\n",
      "[]\n",
      "{'_id': 1,\n",
      " 'courses': [{'code': 'TDT4225',\n",
      "              'name': ' Very Large, Distributed Data Volumes'},\n",
      "             {'code': 'BOI1001', 'name': ' How to become a boi or boierinnaa'}],\n",
      " 'name': 'Bobby'}\n",
      "{'_id': 2,\n",
      " 'courses': [{'code': 'TDT02', 'name': ' Advanced, Distributed Systems'}],\n",
      " 'name': 'Bobby'}\n",
      "{'_id': 3, 'name': 'Bobby'}\n",
      "[]\n",
      "\n",
      "-----------------------------------------------\n",
      "Connection to my_db-db is closed\n"
     ]
    }
   ],
   "source": [
    "class ExampleProgram:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.connection = DbConnector()\n",
    "        self.client = self.connection.client\n",
    "        self.db = self.connection.db\n",
    "\n",
    "    def create_coll(self, collection_name):\n",
    "        collection = self.db.create_collection(collection_name)    \n",
    "        print('Created collection: ', collection)\n",
    "\n",
    "    def insert_documents(self, collection_name):\n",
    "        docs = [\n",
    "            {\n",
    "                \"_id\": 1,\n",
    "                \"name\": \"Bobby\",\n",
    "                \"courses\": \n",
    "                    [\n",
    "                    {'code':'TDT4225', 'name': ' Very Large, Distributed Data Volumes'},\n",
    "                    {'code':'BOI1001', 'name': ' How to become a boi or boierinnaa'}\n",
    "                    ] \n",
    "            },\n",
    "            {\n",
    "                \"_id\": 2,\n",
    "                \"name\": \"Bobby\",\n",
    "                \"courses\": \n",
    "                    [\n",
    "                    {'code':'TDT02', 'name': ' Advanced, Distributed Systems'},\n",
    "                    ] \n",
    "            },\n",
    "            {\n",
    "                \"_id\": 3,\n",
    "                \"name\": \"Bobby\",\n",
    "            }\n",
    "        ]  \n",
    "        collection = self.db[collection_name]\n",
    "        collection.insert_many(docs)\n",
    "        \n",
    "    def fetch_documents(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        documents = collection.find({})\n",
    "        for doc in documents: \n",
    "            pprint(doc)\n",
    "        \n",
    "\n",
    "    def drop_coll(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        collection.drop()\n",
    "\n",
    "        \n",
    "    def show_coll(self):\n",
    "        collections = self.client['test'].list_collection_names()\n",
    "        print(collections)\n",
    "         \n",
    "\n",
    "\n",
    "def main():\n",
    "    program = None\n",
    "    try:\n",
    "        program = ExampleProgram()\n",
    "        program.create_coll(collection_name=\"Person\")\n",
    "        program.show_coll()\n",
    "        program.insert_documents(collection_name=\"Person\")\n",
    "        program.fetch_documents(collection_name=\"Person\")\n",
    "        program.drop_coll(collection_name=\"Person\")\n",
    "        # program.drop_coll(collection_name='person')\n",
    "        # program.drop_coll(collection_name='users')\n",
    "        # Check that the table is dropped\n",
    "        program.show_coll()\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to use database:\", e)\n",
    "    finally:\n",
    "        if program:\n",
    "            program.connection.close_connection()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/dataset/dataset/Data\n"
     ]
    }
   ],
   "source": [
    "# we go to the Data folder\n",
    "os.chdir(\"..\")\n",
    "path = os.getcwd()\n",
    "os.chdir(\"/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/dataset/dataset/Data\")\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_user_dataframe():\n",
    "    # we get the list of all the diferent directory names (users ids) and we sort the list\n",
    "    usersID =  os.listdir()\n",
    "    usersID.sort()\n",
    "\n",
    "    # we go back to the dataset directory and we read the labeled_ids.txt file\n",
    "    os.chdir(\"..\")\n",
    "    with open('labeled_ids.txt') as f:\n",
    "        labeled_ids = f.readlines()\n",
    "    f.close()\n",
    "    os.chdir(\"/Users/victoriastasik/Documents/Very_large_distributed_data_volumes_exercise3/dataset/dataset/Data\")\n",
    "        \n",
    "    # we delete the \\n in each string\n",
    "    for i in range (0, len(labeled_ids)):\n",
    "        labeled_ids[i] = labeled_ids[i].strip()\n",
    "\n",
    "    # we check if each user has a label or not and we save the info in a list\n",
    "    # the indexes of has_labels and id lists are correponding\n",
    "    has_labels = []\n",
    "    for i in usersID : \n",
    "        if i in labeled_ids : \n",
    "            has_labels.append(True)\n",
    "        else:\n",
    "            has_labels.append(False)\n",
    "\n",
    "    user_table = {'user_id': usersID, 'has_labels': has_labels}\n",
    "    user_dataframe = pd.DataFrame(user_table)\n",
    "    return user_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_labels_txt_file() :\n",
    "    user_id_list = []\n",
    "    transportation_mode_list = []\n",
    "    start_date_time_list = []\n",
    "    end_date_time_list = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(os.getcwd()):\n",
    "        for filename in filenames:\n",
    "            # if there is a labels.txt file, we save the info\n",
    "            if filename.endswith('.txt'):\n",
    "                try :\n",
    "                    with open(os.path.join(dirpath, filename)) as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                        # skip the header\n",
    "                        lines = lines[1:]\n",
    "\n",
    "                        for line in lines:\n",
    "                            \n",
    "                            # we get the info of one line\n",
    "                            data = line.split()\n",
    "\n",
    "                            # we save each information into the correct list\n",
    "                            last_directory_name = os.path.basename(dirpath)\n",
    "                            user_id_list.append(last_directory_name)\n",
    "\n",
    "                            transportation_mode_list.append(data[4])\n",
    "\n",
    "                            start_date = data[0]\n",
    "                            start_time = data[1]\n",
    "                            end_date = data[2]\n",
    "                            end_time = data[3]\n",
    "                            start_datetime_str = start_date + \" \" + start_time\n",
    "                            end_datetime_str = end_date + \" \" + end_time\n",
    "                            start_datetime_str = start_datetime_str.replace('/', '-')\n",
    "                            end_datetime_str = end_datetime_str.replace('/', '-')\n",
    "                            combined_start_datetime = datetime.strptime(start_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "                            combined_end_datetime = datetime.strptime(end_datetime_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                            start_date_time_list.append(combined_start_datetime)\n",
    "                            end_date_time_list.append(combined_end_datetime)\n",
    "                    f.close()\n",
    "\n",
    "                # error handling\n",
    "                except FileNotFoundError:\n",
    "                    print(f\"the file {filename} doesn't exist.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "\n",
    "    return user_id_list, transportation_mode_list, start_date_time_list, end_date_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_from_plt_file() :\n",
    "    activity = 0\n",
    "    activity_id_list = []\n",
    "    user_id_list = []\n",
    "    lat_list = []\n",
    "    long_list = []\n",
    "    altitude_list = []\n",
    "    date_days_list = []\n",
    "    current_date_time_list = []\n",
    "    start_date_time_list = []\n",
    "    end_date_time_list = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(os.getcwd()):\n",
    "        for filename in filenames:\n",
    "\n",
    "            # we get the information of each plt file\n",
    "            if filename.endswith('.plt'):\n",
    "                try :\n",
    "                    with open(os.path.join(dirpath, filename)) as f:\n",
    "                        lines = f.readlines()\n",
    "\n",
    "                        # skip the first 6 lines\n",
    "                        lines = lines[6:]\n",
    "\n",
    "                        # check the length of the plt file\n",
    "                        if len(lines) <= 2500:\n",
    "\n",
    "                            # we get the start and end date of each plt file\n",
    "                            start_line= lines[0].split(',')\n",
    "                            start_date = start_line[5]\n",
    "                            start_time = start_line[6]\n",
    "                            start_datetime = start_date + ' ' + start_time\n",
    "                            start_datetime = start_datetime.rstrip('\\n')\n",
    "                            start_datetime = datetime.strptime(start_datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "                            end_line = lines[len(lines)-1].split(',')\n",
    "                            end_date = end_line[5]\n",
    "                            end_time = end_line[6]\n",
    "                            end_datetime = end_date + ' ' + end_time\n",
    "                            end_datetime = end_datetime.rstrip('\\n')\n",
    "                            end_datetime = datetime.strptime(end_datetime, \"%Y-%m-%d %H:%M:%S\")\n",
    "    \n",
    "                            # we save the information of each line of the plt file\n",
    "                            for line in lines:\n",
    "\n",
    "                                data = line.split(',')\n",
    "\n",
    "                                activity_id_list.append(activity)\n",
    "\n",
    "                                parent_directory = os.path.dirname(dirpath)\n",
    "                                directory_name = os.path.basename(parent_directory) \n",
    "                                user_id_list.append(directory_name)\n",
    "\n",
    "                                lat_list.append(float(data[0]))\n",
    "                                long_list.append(float(data[1]))\n",
    "                                altitude_list.append(int(float(data[3])))\n",
    "                                date_days_list.append(float(data[4]))\n",
    "\n",
    "                                date = data[5]\n",
    "                                time = data[6]\n",
    "                                datetime_draft = date + ' ' + time\n",
    "                                datetime_draft = datetime_draft.rstrip('\\n')\n",
    "                                combined_datetime = datetime.strptime(datetime_draft, \"%Y-%m-%d %H:%M:%S\")\n",
    "                                current_date_time_list.append(combined_datetime)\n",
    "\n",
    "                                start_date_time_list.append(start_datetime)\n",
    "                                end_date_time_list.append(end_datetime)\n",
    "\n",
    "                    f.close()\n",
    "                    activity +=1\n",
    "                  \n",
    "                except FileNotFoundError:\n",
    "                    print(f\"the file {filename} doesn't exist.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                    \n",
    "    return activity_id_list, user_id_list, lat_list, long_list, altitude_list, date_days_list, current_date_time_list, start_date_time_list, end_date_time_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def creating_activity_and_trackpoint_dataframe():    \n",
    "    # getting info from txt files\n",
    "    user_id_list, transportation_mode_list, start_date_time_list, end_date_time_list = get_info_from_labels_txt_file()\n",
    "    labels_txt = {'user_id': user_id_list, 'transportation_mode': transportation_mode_list, 'start_datetime': start_date_time_list, 'end_datetime': end_date_time_list}\n",
    "    labels_txt_df = pd.DataFrame(labels_txt)\n",
    "\n",
    "    # getting info from plt files\n",
    "    activity_id_list, user_id_list_2, lat_list, long_list, altitude_list, date_days_list, current_date_time_list, start_date_time_list2, end_date_time_list2 = get_info_from_plt_file()\n",
    "    plt = {'activity_id': activity_id_list, 'user_id': user_id_list_2, 'lat': lat_list, 'long': long_list, 'altitude': altitude_list, 'date_days': date_days_list, \n",
    "           'current_date_time': current_date_time_list, 'start_datetime': start_date_time_list2, 'end_datetime': end_date_time_list2}\n",
    "    plt_df = pd.DataFrame(plt)\n",
    "\n",
    "    # merging both dataframes\n",
    "    merged_df = pd.merge(plt_df, labels_txt_df, on=['user_id', 'start_datetime', 'end_datetime'], how='left')\n",
    "\n",
    "    # creating activity table\n",
    "    activity_table = merged_df[['activity_id','user_id', 'transportation_mode', 'start_datetime', 'end_datetime']]\n",
    "    activity_table = activity_table.fillna(\"missing\")\n",
    "    activity_table['start_datetime'] = activity_table['start_datetime'].astype(str) # converting into string to be able to insert into the sql table\n",
    "    activity_table['end_datetime'] = activity_table['end_datetime'].astype(str)\n",
    "    activity_table = activity_table.drop_duplicates()\n",
    "\n",
    "    # creating trackpoint table\n",
    "    trackpoint_table = merged_df[['activity_id','lat', 'long', 'altitude', 'date_days', 'current_date_time']]\n",
    "    trackpoint_table.rename(columns={'current_date_time': 'date_time'}, inplace=True)\n",
    "    trackpoint_table['id'] = range(1, len(trackpoint_table) + 1)\n",
    "    trackpoint_table = trackpoint_table[['id'] + [col for col in trackpoint_table.columns if col != 'id']]\n",
    "    trackpoint_table['date_time'] = trackpoint_table['date_time'].astype(str)\n",
    "\n",
    "    return activity_table, trackpoint_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task1:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.connection = DbConnector()\n",
    "        self.client = self.connection.client\n",
    "        self.db = self.connection.db\n",
    "\n",
    "    def create_coll(self, collection_name):\n",
    "        collection = self.db.create_collection(collection_name)    \n",
    "        print('Created collection: ', collection)\n",
    "\n",
    "    def insert_activity_data(self, user_table, activity_table):\n",
    "        merged_df = pd.merge(user_table, activity_table, on=['user_id'])\n",
    "\n",
    "        result_list = []\n",
    "        for index, row in merged_df.iterrows():\n",
    "            user_info = {'user_id': row['user_id'], 'has_labels': row['has_labels']}\n",
    "            entry = {\n",
    "                '_id': index,\n",
    "                'activity_id': row['activity_id'],\n",
    "                'user_info': user_info,\n",
    "                'transportation_mode': row['transportation_mode'],\n",
    "                'start_datetime': row['start_datetime'],\n",
    "                'end_datetime': row['end_datetime']\n",
    "            }\n",
    "            result_list.append(entry)\n",
    "        \n",
    "        collection = self.db[\"Activity\"]\n",
    "        collection.insert_many(result_list)\n",
    "\n",
    "    def insert_trackpoint_data(self, trackpoint_table):\n",
    "        \n",
    "        result_list = []\n",
    "        for index, row in trackpoint_table.iterrows():\n",
    "            entry = {\n",
    "                '_id': row['id'],\n",
    "                'activity_id': row['activity_id'],\n",
    "                'lat': row['lat'],\n",
    "                'lon': row['long'],\n",
    "                'altitude': row['altitude'],\n",
    "                'date_days': row['date_days'],\n",
    "                'date_time': row['date_time']\n",
    "            }\n",
    "            result_list.append(entry)\n",
    "        \n",
    "        collection = self.db[\"TrackPoint\"]\n",
    "        collection.insert_many(result_list)\n",
    "\n",
    "    def compute_aggregated_query(self, collection_name, pipeline):\n",
    "        collection = self.db[collection_name]\n",
    "        result = collection.aggregate(pipeline)\n",
    "        for i in result: \n",
    "            pprint(i)\n",
    "        \n",
    "    def fetch_documents(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        documents = collection.find({})\n",
    "        for doc in documents: \n",
    "            pprint(doc)\n",
    "        \n",
    "    def drop_coll(self, collection_name):\n",
    "        collection = self.db[collection_name]\n",
    "        collection.drop()\n",
    "\n",
    "        \n",
    "    def show_coll(self):\n",
    "        collections = self.client['my_db'].list_collection_names()\n",
    "        print(collections)         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to the database: my_db\n",
      "-----------------------------------------------\n",
      "\n",
      "\n",
      "-----------------------------------------------\n",
      "Connection to my_db-db is closed\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    program = None\n",
    "    try:\n",
    "        activity_table, trackpoint_table = creating_activity_and_trackpoint_dataframe()\n",
    "        user_table = creating_user_dataframe()\n",
    "        \n",
    "        program = Task1()\n",
    "\n",
    "        program.drop_coll(collection_name=\"Activity\")\n",
    "        program.drop_coll(collection_name=\"TrackPoint\")\n",
    "\n",
    "        program.create_coll(collection_name=\"Activity\")\n",
    "        program.create_coll(collection_name=\"TrackPoint\")\n",
    "        program.show_coll()\n",
    "        \n",
    "        program.insert_activity_data(user_table, activity_table)\n",
    "        program.insert_trackpoint_data(trackpoint_table)\n",
    "\n",
    "        program.fetch_documents(collection_name=\"Activity\")\n",
    "        program.fetch_documents(collection_name=\"TrackPoint\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"ERROR: Failed to use database:\", e)\n",
    "    finally:\n",
    "        if program:\n",
    "            program.connection.close_connection()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are connected to the database: my_db\n",
      "-----------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "connection = DbConnector()\n",
    "client = connection.client\n",
    "db = connection.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': '128', 'count': 2102}\n",
      "{'_id': '153', 'count': 1793}\n",
      "{'_id': '025', 'count': 715}\n",
      "{'_id': '163', 'count': 704}\n",
      "{'_id': '062', 'count': 691}\n",
      "{'_id': '144', 'count': 563}\n",
      "{'_id': '041', 'count': 399}\n",
      "{'_id': '085', 'count': 364}\n",
      "{'_id': '004', 'count': 346}\n",
      "{'_id': '140', 'count': 345}\n",
      "{'_id': '167', 'count': 320}\n",
      "{'_id': '068', 'count': 280}\n",
      "{'_id': '017', 'count': 265}\n",
      "{'_id': '003', 'count': 261}\n",
      "{'_id': '014', 'count': 236}\n",
      "{'_id': '126', 'count': 215}\n",
      "{'_id': '030', 'count': 210}\n",
      "{'_id': '112', 'count': 208}\n",
      "{'_id': '011', 'count': 201}\n",
      "{'_id': '039', 'count': 198}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$user_info.user_id\",\n",
    "            \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": {\"count\": -1}\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 20\n",
    "    }\n",
    "]\n",
    "result = list(db.Activity.aggregate(pipeline))\n",
    "\n",
    "for doc in result:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 2008, 'count': 5895}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"year\": { \"$year\": { \"$dateFromString\": { \"dateString\": \"$start_datetime\" } }}\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$year\",\n",
    "            \"count\": { \"$sum\": 1 }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": { \"count\": -1 }\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "result = list(db.Activity.aggregate(pipeline))\n",
    "\n",
    "for doc in result:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 6b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 2009, 'total_hours': 11612.423888888889}\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "    {\n",
    "        \"$addFields\": {\n",
    "            \"start_datetime\": {\n",
    "                \"$dateFromString\": {\n",
    "                    \"dateString\": \"$start_datetime\",\n",
    "                    \"format\": \"%Y-%m-%d %H:%M:%S\"\n",
    "                }\n",
    "            },\n",
    "            \"end_datetime\": {\n",
    "                \"$dateFromString\": {\n",
    "                    \"dateString\": \"$end_datetime\",\n",
    "                    \"format\": \"%Y-%m-%d %H:%M:%S\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$project\": {\n",
    "            \"year\": { \"$year\": \"$start_datetime\" },\n",
    "            \"duration\": {\n",
    "                \"$divide\": [\n",
    "                    { \"$subtract\": [\"$end_datetime\", \"$start_datetime\"] },\n",
    "                    1000 * 60 * 60\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$group\": {\n",
    "            \"_id\": \"$year\",\n",
    "            \"total_hours\": { \"$sum\": \"$duration\" }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"$sort\": { \"total_hours\": -1 }\n",
    "    },\n",
    "    {\n",
    "        \"$limit\": 1\n",
    "    }\n",
    "]\n",
    "\n",
    "result = list(db.Activity.aggregate(pipeline))\n",
    "\n",
    "for doc in result:\n",
    "    pprint(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_id: 135, invalid_activities: 21\n",
      "user_id: 132, invalid_activities: 3\n",
      "user_id: 104, invalid_activities: 351\n",
      "user_id: 103, invalid_activities: 44\n",
      "user_id: 168, invalid_activities: 36\n",
      "user_id: 157, invalid_activities: 65\n",
      "user_id: 150, invalid_activities: 60\n",
      "user_id: 159, invalid_activities: 7\n",
      "user_id: 166, invalid_activities: 4\n",
      "user_id: 161, invalid_activities: 7\n",
      "user_id: 102, invalid_activities: 16\n",
      "user_id: 105, invalid_activities: 35\n",
      "user_id: 133, invalid_activities: 18\n",
      "user_id: 134, invalid_activities: 83\n",
      "user_id: 158, invalid_activities: 28\n",
      "user_id: 167, invalid_activities: 334\n",
      "user_id: 151, invalid_activities: 4\n",
      "user_id: 169, invalid_activities: 12\n",
      "user_id: 024, invalid_activities: 75\n",
      "user_id: 023, invalid_activities: 30\n",
      "user_id: 015, invalid_activities: 138\n",
      "user_id: 012, invalid_activities: 163\n",
      "user_id: 079, invalid_activities: 3\n",
      "user_id: 046, invalid_activities: 26\n",
      "user_id: 041, invalid_activities: 389\n",
      "user_id: 048, invalid_activities: 5\n",
      "user_id: 077, invalid_activities: 11\n",
      "user_id: 083, invalid_activities: 56\n",
      "user_id: 084, invalid_activities: 230\n",
      "user_id: 070, invalid_activities: 16\n",
      "user_id: 013, invalid_activities: 69\n",
      "user_id: 014, invalid_activities: 328\n",
      "user_id: 022, invalid_activities: 191\n",
      "user_id: 025, invalid_activities: 426\n",
      "user_id: 071, invalid_activities: 64\n",
      "user_id: 085, invalid_activities: 384\n",
      "user_id: 082, invalid_activities: 65\n",
      "user_id: 076, invalid_activities: 33\n",
      "user_id: 040, invalid_activities: 52\n",
      "user_id: 078, invalid_activities: 26\n",
      "user_id: 047, invalid_activities: 19\n",
      "user_id: 065, invalid_activities: 46\n",
      "user_id: 091, invalid_activities: 110\n",
      "user_id: 096, invalid_activities: 67\n",
      "user_id: 062, invalid_activities: 397\n",
      "user_id: 054, invalid_activities: 8\n",
      "user_id: 053, invalid_activities: 39\n",
      "user_id: 098, invalid_activities: 25\n",
      "user_id: 038, invalid_activities: 266\n",
      "user_id: 007, invalid_activities: 136\n",
      "user_id: 000, invalid_activities: 445\n",
      "user_id: 009, invalid_activities: 114\n",
      "user_id: 036, invalid_activities: 126\n",
      "user_id: 031, invalid_activities: 7\n",
      "user_id: 052, invalid_activities: 138\n",
      "user_id: 099, invalid_activities: 113\n",
      "user_id: 055, invalid_activities: 28\n",
      "user_id: 063, invalid_activities: 27\n",
      "user_id: 097, invalid_activities: 27\n",
      "user_id: 090, invalid_activities: 15\n",
      "user_id: 064, invalid_activities: 10\n",
      "user_id: 030, invalid_activities: 511\n",
      "user_id: 008, invalid_activities: 33\n",
      "user_id: 037, invalid_activities: 279\n",
      "user_id: 001, invalid_activities: 116\n",
      "user_id: 039, invalid_activities: 427\n",
      "user_id: 006, invalid_activities: 49\n",
      "user_id: 174, invalid_activities: 188\n",
      "user_id: 180, invalid_activities: 2\n",
      "user_id: 173, invalid_activities: 11\n",
      "user_id: 145, invalid_activities: 17\n",
      "user_id: 142, invalid_activities: 200\n",
      "user_id: 129, invalid_activities: 41\n",
      "user_id: 111, invalid_activities: 251\n",
      "user_id: 118, invalid_activities: 31\n",
      "user_id: 127, invalid_activities: 8\n",
      "user_id: 144, invalid_activities: 551\n",
      "user_id: 172, invalid_activities: 21\n",
      "user_id: 181, invalid_activities: 49\n",
      "user_id: 175, invalid_activities: 20\n",
      "user_id: 121, invalid_activities: 15\n",
      "user_id: 119, invalid_activities: 79\n",
      "user_id: 126, invalid_activities: 242\n",
      "user_id: 110, invalid_activities: 48\n",
      "user_id: 128, invalid_activities: 1288\n",
      "user_id: 117, invalid_activities: 3\n",
      "user_id: 153, invalid_activities: 1154\n",
      "user_id: 154, invalid_activities: 28\n",
      "user_id: 162, invalid_activities: 27\n",
      "user_id: 165, invalid_activities: 4\n",
      "user_id: 131, invalid_activities: 59\n",
      "user_id: 136, invalid_activities: 34\n",
      "user_id: 109, invalid_activities: 9\n",
      "user_id: 100, invalid_activities: 5\n",
      "user_id: 107, invalid_activities: 2\n",
      "user_id: 138, invalid_activities: 27\n",
      "user_id: 164, invalid_activities: 20\n",
      "user_id: 163, invalid_activities: 559\n",
      "user_id: 155, invalid_activities: 70\n",
      "user_id: 152, invalid_activities: 4\n",
      "user_id: 106, invalid_activities: 66\n",
      "user_id: 139, invalid_activities: 50\n",
      "user_id: 101, invalid_activities: 166\n",
      "user_id: 108, invalid_activities: 28\n",
      "user_id: 130, invalid_activities: 18\n",
      "user_id: 089, invalid_activities: 116\n",
      "user_id: 042, invalid_activities: 118\n",
      "user_id: 045, invalid_activities: 19\n",
      "user_id: 087, invalid_activities: 3\n",
      "user_id: 073, invalid_activities: 30\n",
      "user_id: 074, invalid_activities: 19\n",
      "user_id: 080, invalid_activities: 8\n",
      "user_id: 020, invalid_activities: 27\n",
      "user_id: 027, invalid_activities: 14\n",
      "user_id: 018, invalid_activities: 97\n",
      "user_id: 011, invalid_activities: 38\n",
      "user_id: 016, invalid_activities: 61\n",
      "user_id: 029, invalid_activities: 37\n",
      "user_id: 081, invalid_activities: 42\n",
      "user_id: 075, invalid_activities: 8\n",
      "user_id: 072, invalid_activities: 6\n",
      "user_id: 086, invalid_activities: 19\n",
      "user_id: 044, invalid_activities: 83\n",
      "user_id: 088, invalid_activities: 13\n",
      "user_id: 043, invalid_activities: 73\n",
      "user_id: 017, invalid_activities: 493\n",
      "user_id: 028, invalid_activities: 91\n",
      "user_id: 010, invalid_activities: 154\n",
      "user_id: 026, invalid_activities: 51\n",
      "user_id: 019, invalid_activities: 65\n",
      "user_id: 021, invalid_activities: 72\n",
      "user_id: 003, invalid_activities: 849\n",
      "user_id: 004, invalid_activities: 1173\n",
      "user_id: 032, invalid_activities: 37\n",
      "user_id: 035, invalid_activities: 124\n",
      "user_id: 095, invalid_activities: 20\n",
      "user_id: 061, invalid_activities: 41\n",
      "user_id: 066, invalid_activities: 40\n",
      "user_id: 092, invalid_activities: 315\n",
      "user_id: 059, invalid_activities: 8\n",
      "user_id: 050, invalid_activities: 17\n",
      "user_id: 057, invalid_activities: 60\n",
      "user_id: 068, invalid_activities: 327\n",
      "user_id: 034, invalid_activities: 241\n",
      "user_id: 033, invalid_activities: 3\n",
      "user_id: 005, invalid_activities: 127\n",
      "user_id: 002, invalid_activities: 296\n",
      "user_id: 056, invalid_activities: 11\n",
      "user_id: 069, invalid_activities: 65\n",
      "user_id: 051, invalid_activities: 146\n",
      "user_id: 093, invalid_activities: 4\n",
      "user_id: 067, invalid_activities: 83\n",
      "user_id: 058, invalid_activities: 25\n",
      "user_id: 060, invalid_activities: 1\n",
      "user_id: 094, invalid_activities: 60\n",
      "user_id: 112, invalid_activities: 140\n",
      "user_id: 115, invalid_activities: 183\n",
      "user_id: 123, invalid_activities: 21\n",
      "user_id: 124, invalid_activities: 12\n",
      "user_id: 170, invalid_activities: 2\n",
      "user_id: 141, invalid_activities: 1\n",
      "user_id: 146, invalid_activities: 28\n",
      "user_id: 179, invalid_activities: 42\n",
      "user_id: 125, invalid_activities: 69\n",
      "user_id: 122, invalid_activities: 21\n",
      "user_id: 114, invalid_activities: 4\n",
      "user_id: 113, invalid_activities: 1\n",
      "user_id: 147, invalid_activities: 64\n",
      "user_id: 140, invalid_activities: 195\n",
      "user_id: 176, invalid_activities: 24\n",
      "user_id: 171, invalid_activities: 14\n"
     ]
    }
   ],
   "source": [
    "pipeline = [\n",
    "               {\n",
    "            \"$sort\": { \"trackpoint_id\": 1 }\n",
    "        },\n",
    "        {\n",
    "            \"$group\": {\n",
    "            \"_id\": \"$activity_id\",\n",
    "            \"list_dates\": {\"$push\": \"$date_time\"},\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$lookup\": {\n",
    "            \"from\": \"Activity\",\n",
    "            \"localField\": \"_id\",\n",
    "            \"foreignField\": \"activity_id\",\n",
    "            \"as\": \"activity\"\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"$project\": {\n",
    "                \"user_id\" : \"$activity.user_info.user_id\",\n",
    "                \"activity_id\": \"$_id\",\n",
    "                \"list_dates\": \"$list_dates\"\n",
    "        }\n",
    "        }\n",
    "        ]\n",
    "result = list(db.TrackPoint.aggregate(pipeline))\n",
    "\n",
    "def is_invalid_timestamp(timestamp1, timestamp2):\n",
    "    time_format = '%Y-%m-%d %H:%M:%S'\n",
    "    dt1 = datetime.strptime(timestamp1, time_format)\n",
    "    dt2 = datetime.strptime(timestamp2, time_format)\n",
    "    return abs((dt2 - dt1).total_seconds()) >= 300 \n",
    "\n",
    "invalid_activities = {}\n",
    "\n",
    "for item in result:\n",
    "    user_id = item['user_id'][0]\n",
    "    trackpoints = item['list_dates']\n",
    "    invalid_count = 0\n",
    "    \n",
    "    for i in range(1, len(trackpoints)):\n",
    "        if is_invalid_timestamp(trackpoints[i - 1], trackpoints[i]):\n",
    "            invalid_count += 1\n",
    "    \n",
    "    if invalid_count > 0:\n",
    "        if user_id in invalid_activities:\n",
    "            invalid_activities[user_id] += invalid_count\n",
    "        else:\n",
    "            invalid_activities[user_id] = invalid_count\n",
    "\n",
    "for user_id, count in invalid_activities.items():\n",
    "    print(f\"user_id: {user_id}, invalid_activities: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-----------------------------------------------\n",
      "Connection to my_db-db is closed\n"
     ]
    }
   ],
   "source": [
    "connection.close_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
